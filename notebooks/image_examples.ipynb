{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ..[image] --extra-index-url \"https://download.pytorch.org/whl/cpu/torch_stable.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from torchvision.datasets import Imagenette\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "try:\n",
    "    dataset = Imagenette(root=\"data/\", download=True, split=\"val\", size=\"320px\")\n",
    "except RuntimeError:\n",
    "    print(\"Dataset already downloaded\")\n",
    "    dataset = Imagenette(root=\"data/\", download=False, split=\"val\", size=\"320px\")\n",
    "\n",
    "random.seed(10)\n",
    "dataset = random.sample([x for x, _ in dataset], k=5)\n",
    "\n",
    "\n",
    "def generate_distortions_dataset(dataset):\n",
    "    distorted = []\n",
    "    for img in dataset:\n",
    "        distorted.append(img)\n",
    "        distorted.append(F.adjust_brightness(img, 2))\n",
    "        distorted.append(F.adjust_gamma(img, 2))\n",
    "        distorted.append(F.adjust_saturation(img, 0.1))\n",
    "        distorted.append(F.gaussian_blur(img, 21))\n",
    "    distorted = [np.asarray(img) for img in distorted]\n",
    "    return distorted\n",
    "\n",
    "\n",
    "images = generate_distortions_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_instances_score(images: list[np.ndarray], metric: str, scores: list[float], n_cols: int = 5):\n",
    "    n_rows = len(images) // n_cols\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(n_cols * 4, n_rows * 3))\n",
    "    for ax, image, score in zip(axs.flat, images, scores):\n",
    "        ax.imshow(image)\n",
    "        ax.set_title(f\"{metric}: {score:.2f}\")\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_aspect(\"auto\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Validation\n",
    "\n",
    "In the image modality, the pymdma package provides two types of input validation:\n",
    "- **no-reference**: The image is validated without any reference image.\n",
    "- **reference**: The image is validated with a reference image.\n",
    "\n",
    "This section will demonstrate how to use the input validation functions from both types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Reference Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymdma.image.measures.input_val import CLIPIQA\n",
    "\n",
    "clip = CLIPIQA(img_size=320)  # instanciate the metric class\n",
    "clip_result = clip.compute(images)  # compute the metric\n",
    "_dataset_level, instance_level = clip_result.value  # fetch the instance level results\n",
    "\n",
    "plot_instances_score(images, \"CLIPIQA\", instance_level, n_cols=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymdma.image.measures.input_val import Tenengrad\n",
    "\n",
    "tenengrad = Tenengrad()  # sharpness metric\n",
    "sharpness = tenengrad.compute(images)  # compute on RGB images\n",
    "\n",
    "_dataset_level, instance_level = sharpness.value\n",
    "\n",
    "plot_instances_score(images, \"Sharpness\", instance_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymdma.image.measures.input_val import Brightness\n",
    "\n",
    "brightness = Brightness()\n",
    "brightness_result = brightness.compute(images)\n",
    "_dataset_level, instance_level = brightness_result.value\n",
    "plot_instances_score(images, \"Brightness\", instance_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymdma.image.measures.input_val import Colorfulness\n",
    "\n",
    "colorfulness = Colorfulness()\n",
    "colorfulness_result = colorfulness.compute(images)\n",
    "_dataset_level, instance_level = colorfulness_result.value\n",
    "plot_instances_score(images, \"Colorfulness\", instance_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ploting the metric results\n",
    "\n",
    "We provide a simple method in the `MetricResult` class to easily plot the results of the metrics. The method `plot()` will plot the results of the metrics in the format specified by the `plot_params` attribute in the `MetricResult` class. The `plot_params` attribute is a dictionary that contains the parameters to be used in the plot. If this attribute is not set, the method will default to a bar plot.\n",
    "\n",
    "You can provide a title for the plot when calling this method, as well as an axis is which you wish to plot the results (helpfull when plotting multiple metrics in the same plot). In addition, you can provide a set of `plot_params` to be used directly by matplotlib's plotting functions.\n",
    "\n",
    "> **Note**: You also have access to the values of the metrics via the `values` attribute in the `MetricResult` class. You can use these values to plot the results using your own plotting functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "brightness_result.plot(\"Brightness\")  # plot the results from the result object\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))  # create a figure with two subplots\n",
    "brightness_result.plot(\"Brightness\", bins=5, ax=axs[0])  # plot the BRISQUE histogram on the first subplot\n",
    "colorfulness_result.plot(\"Colorfulness\", bins=5, ax=axs[1])  # plot the CLIPIQA histogram on the second subplot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Reference Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "N_DISTORTIONS = 5\n",
    "\n",
    "reference = []\n",
    "for img in dataset:\n",
    "    reference += [np.asarray(img)] * N_DISTORTIONS\n",
    "\n",
    "\n",
    "def generate_full_ref_dataset(dataset):\n",
    "    distorted = []\n",
    "    for idx, img in enumerate(dataset):\n",
    "        img = np.asarray(img)\n",
    "        for var in np.linspace(1, 10, N_DISTORTIONS):\n",
    "            gauss = np.random.normal(0, var, img.shape)\n",
    "            distorted.append((img + gauss).astype(np.uint8))\n",
    "    return [np.asarray(x) for x in distorted]\n",
    "\n",
    "\n",
    "distorted = generate_full_ref_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymdma.image.measures.input_val import PSNR\n",
    "\n",
    "psnr = PSNR()\n",
    "psnr_result = psnr.compute(reference, distorted)\n",
    "_, instance_level = psnr_result.value\n",
    "\n",
    "for i in range(0, len(instance_level), N_DISTORTIONS):\n",
    "    plot_instances_score(distorted[i : i + N_DISTORTIONS], \"PSNR\", instance_level[i : i + N_DISTORTIONS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymdma.image.measures.input_val import MSSIM\n",
    "\n",
    "\n",
    "def generate_full_ref_dataset(dataset):\n",
    "    distorted = []\n",
    "    for idx, img in enumerate(dataset):\n",
    "        img = np.asarray(img)\n",
    "        for size in range(N_DISTORTIONS + 1, 1, -1):\n",
    "            dst = img.copy()\n",
    "            dst = cv.rectangle(dst, (20, 20), (img.shape[1] // size, img.shape[0] // size), (0, 0, 0), -1)\n",
    "            distorted.append((dst).astype(np.uint8))\n",
    "    return [np.asarray(x) for x in distorted]\n",
    "\n",
    "\n",
    "distorted = generate_full_ref_dataset(dataset)\n",
    "\n",
    "mssim = MSSIM(kernel_size=15)\n",
    "mssim_result = mssim.compute(reference, distorted)\n",
    "_, instance_level = mssim_result.value\n",
    "\n",
    "for i in range(0, len(instance_level), N_DISTORTIONS):\n",
    "    plot_instances_score(distorted[i : i + N_DISTORTIONS], \"MSSIM\", instance_level[i : i + N_DISTORTIONS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Validation\n",
    "\n",
    "The automatic evaluation of synthetically generated images is a common practice in the field of generative AI, and is crucial for the assessment of the quality of large synthetic datasets. This is usually done by comparing the synthetic images to a set of reference images by considering the similarity between the distributions of the two sets. In this section, we will demonstrate how to use the `pymdma` package to evaluate the quality of synthetic images.\n",
    "\n",
    "> **Warning**: Please download the CIFAKE dataset from the following link: [CIFAKE](https://www.kaggle.com/datasets/birdy654/cifake-real-and-ai-generated-synthetic-images) and extract the files to the `data` folder in the root directory of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download CIFAKE dataset from kaggle\n",
    "%pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "kagglehub.config.DEFAULT_CACHE_FOLDER = Path(\"data/.kagglehub\")\n",
    "cifake_path = kagglehub.dataset_download(\"birdy654/cifake-real-and-ai-generated-synthetic-images\")\n",
    "cifake_path = Path(cifake_path)\n",
    "print(\"Downloaded CIFake dataset to \", str(cifake_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from pymdma.image.models.features import ExtractorFactory\n",
    "\n",
    "random.seed(10)\n",
    "\n",
    "cifake_path = cifake_path / \"test\"\n",
    "test_images_ref = Path(cifake_path / \"REAL\")  # real images\n",
    "test_images_synth = Path(cifake_path / \"FAKE\")  # synthetic images\n",
    "\n",
    "images_ref = random.sample([img for img in test_images_ref.iterdir() if img.is_file()], 5000)\n",
    "images_synth = random.sample([img for img in test_images_synth.iterdir() if img.is_file()], 5000)\n",
    "\n",
    "extractor = ExtractorFactory.model_from_name(name=\"dino_vits8\")\n",
    "ref_features = extractor.extract_features_from_files(images_ref)\n",
    "synth_features = extractor.extract_features_from_files(images_synth)\n",
    "\n",
    "print(\"Reference features shape:\", ref_features.shape)\n",
    "print(\"Synthetic features shape:\", synth_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "\n",
    "umap = UMAP(n_components=2, random_state=10, n_jobs=1)\n",
    "real_feats_2d = umap.fit_transform(ref_features)\n",
    "fake_feats_2d = umap.transform(synth_features)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(real_feats_2d[:, 0], real_feats_2d[:, 1], s=3, label=\"Real Samples\")\n",
    "plt.scatter(fake_feats_2d[:, 0], fake_feats_2d[:, 1], s=3, label=\"Fake Samples\")\n",
    "plt.title(\"UMAP Features Visualization | Real vs Synthetic\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymdma.image.measures.synthesis_val import ImprovedPrecision, ImprovedRecall\n",
    "\n",
    "ip = ImprovedPrecision(k=5)\n",
    "ir = ImprovedRecall(k=5)\n",
    "\n",
    "ip_result = ip.compute(ref_features, synth_features)\n",
    "ir_result = ir.compute(ref_features, synth_features)\n",
    "\n",
    "precision_dataset, precision_instance = ip_result.value\n",
    "recall_dataset, recall_instance = ir_result.value\n",
    "\n",
    "print(f\"Precision: {precision_dataset:.2f} | Recall: {recall_dataset:.2f}\")\n",
    "print(f\"Precision: {precision_instance[:20]} | Recall: {recall_instance[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_instances_grid(images: list[np.ndarray], n_cols: int = 25):\n",
    "    n_rows = len(images) // n_cols\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(n_cols * 1, n_rows * 1))\n",
    "    fig.subplots_adjust(hspace=0, wspace=0)\n",
    "    for ax, image in zip(axs.flat, images):\n",
    "        ax.imshow(image)\n",
    "        ax.get_xaxis().set_ticks([])\n",
    "        ax.get_yaxis().set_ticks([])\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_aspect(\"auto\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "random.seed(12)\n",
    "precision_instance = np.array(precision_instance)\n",
    "imprecise_idx = np.argwhere(precision_instance < 1).flatten()\n",
    "precise_idx = np.argwhere(precision_instance >= 1).flatten()\n",
    "\n",
    "precise_samples = random.sample(list(precise_idx), 200)\n",
    "imprecise_samples = random.sample(list(imprecise_idx), 200)\n",
    "precise_samples = [np.asarray(Image.open(images_synth[i])) for i in precise_samples]\n",
    "imprecise_samples = [np.asarray(Image.open(images_synth[i])) for i in imprecise_samples]\n",
    "\n",
    "precise_fig = plot_instances_grid(precise_samples, n_cols=25)\n",
    "precise_fig.suptitle(\"CIFAKE Precise samples\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "imprecise_fig = plot_instances_grid(imprecise_samples, n_cols=25)\n",
    "imprecise_fig.suptitle(\"CIFAKE Imprecise samples\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymdma.image.measures.synthesis_val import GIQA\n",
    "\n",
    "giqa = GIQA()\n",
    "giqa_result = giqa.compute(ref_features, synth_features)\n",
    "\n",
    "giqa_dataset, giqa_instance = giqa_result.value\n",
    "print(f\"Dataset level: {giqa_dataset:.2f}\")\n",
    "print(f\"Instance level: {giqa_instance[:40]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "giqa_result.plot(\"GIQA\", bins=50)\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx = np.argsort(giqa_instance)[::-1][:200]\n",
    "best_samples = [np.asarray(Image.open(images_synth[i])) for i in best_idx]\n",
    "\n",
    "best_fig = plot_instances_grid(best_samples, n_cols=25)\n",
    "best_fig.suptitle(\"CIFAKE Best samples\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "worst_idx = np.argsort(giqa_instance)[:200]\n",
    "worst_samples = [np.asarray(Image.open(images_synth[i])) for i in worst_idx]\n",
    "\n",
    "worst_fig = plot_instances_grid(worst_samples, n_cols=25)\n",
    "worst_fig.suptitle(\"CIFAKE Worst samples\", fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
